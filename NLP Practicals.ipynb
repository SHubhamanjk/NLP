{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "630706b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fcac17",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b722f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"My name is shubham kumar gupta , i am currenly working at google as data scientists. my CTC is 25LPA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8209fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is shubham kumar gupta , i am currenly working at google as data scientists. my CTC is 25LPA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ed935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26dafd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is shubham kumar gupta , i am currenly working at google as data scientists.',\n",
       " 'my CTC is 25LPA']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8df8521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'shubham',\n",
       " 'kumar',\n",
       " 'gupta',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currenly',\n",
       " 'working',\n",
       " 'at',\n",
       " 'google',\n",
       " 'as',\n",
       " 'data',\n",
       " 'scientists',\n",
       " '.',\n",
       " 'my',\n",
       " 'CTC',\n",
       " 'is',\n",
       " '25LPA']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9a0c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'shubham',\n",
       " 'kumar',\n",
       " 'gupta',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currenly',\n",
       " 'working',\n",
       " 'at',\n",
       " 'google',\n",
       " 'as',\n",
       " 'data',\n",
       " 'scientists',\n",
       " '.',\n",
       " 'my',\n",
       " 'CTC',\n",
       " 'is',\n",
       " '25LPA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b061934c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'shubham',\n",
       " 'kumar',\n",
       " 'gupta',\n",
       " ',',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currenly',\n",
       " 'working',\n",
       " 'at',\n",
       " 'google',\n",
       " 'as',\n",
       " 'data',\n",
       " 'scientists.',\n",
       " 'my',\n",
       " 'CTC',\n",
       " 'is',\n",
       " '25LPA']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "token=TreebankWordTokenizer()\n",
    "token.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da01c9",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112d36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11772047",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"Happiness\",  \n",
    "    \"Agreement\",  \n",
    "    \"Understanding\",  \n",
    "    \"Deniable\",  \n",
    "    \"Activation\",  \n",
    "    \"Believable\",  \n",
    "    \"Respectful\",  \n",
    "    \"Manageable\",  \n",
    "    \"Doubtful\",  \n",
    "    \"Interpretation\",  \n",
    "    \"Estimation\",  \n",
    "    \"Preparedness\",  \n",
    "    \"Cooked\",  \n",
    "    \"Successful\",  \n",
    "    \"Consideration\",  \n",
    "    \"Production\",  \n",
    "    \"Approval\",  \n",
    "    \"Realistic\",  \n",
    "    \"Statement\",  \n",
    "    \"Writing\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cc40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PS=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698bd358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness-->happi\n",
      "Agreement-->agreement\n",
      "Understanding-->understand\n",
      "Deniable-->deniabl\n",
      "Activation-->activ\n",
      "Believable-->believ\n",
      "Respectful-->respect\n",
      "Manageable-->manag\n",
      "Doubtful-->doubt\n",
      "Interpretation-->interpret\n",
      "Estimation-->estim\n",
      "Preparedness-->prepared\n",
      "Cooked-->cook\n",
      "Successful-->success\n",
      "Consideration-->consider\n",
      "Production-->product\n",
      "Approval-->approv\n",
      "Realistic-->realist\n",
      "Statement-->statement\n",
      "Writing-->write\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"-->\"+PS.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3dd1c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness-->Happines\n",
      "Agreement-->Agreement\n",
      "Understanding-->Understand\n",
      "Deniable-->Deni\n",
      "Activation-->Activation\n",
      "Believable-->Believ\n",
      "Respectful-->Respectful\n",
      "Manageable-->Manage\n",
      "Doubtful-->Doubtful\n",
      "Interpretation-->Interpretation\n",
      "Estimation-->Estimation\n",
      "Preparedness-->Preparednes\n",
      "Cooked-->Cooked\n",
      "Successful-->Successful\n",
      "Consideration-->Consideration\n",
      "Production-->Production\n",
      "Approval-->Approval\n",
      "Realistic-->Realistic\n",
      "Statement-->Statement\n",
      "Writing-->Writ\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regx=RegexpStemmer(\"ing$|s$|e$|able$\")\n",
    "for word in words:\n",
    "    print(word+\"-->\"+regx.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79d18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness-->happi\n",
      "Agreement-->agreement\n",
      "Understanding-->understand\n",
      "Deniable-->deniabl\n",
      "Activation-->activ\n",
      "Believable-->believ\n",
      "Respectful-->respect\n",
      "Manageable-->manag\n",
      "Doubtful-->doubt\n",
      "Interpretation-->interpret\n",
      "Estimation-->estim\n",
      "Preparedness-->prepared\n",
      "Cooked-->cook\n",
      "Successful-->success\n",
      "Consideration-->consider\n",
      "Production-->product\n",
      "Approval-->approv\n",
      "Realistic-->realist\n",
      "Statement-->statement\n",
      "Writing-->write\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss=SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(word+\"-->\"+ss.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23d6ca",
   "metadata": {},
   "source": [
    "# Lemetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb8d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness-->Happiness\n",
      "Agreement-->Agreement\n",
      "Understanding-->Understanding\n",
      "Deniable-->Deniable\n",
      "Activation-->Activation\n",
      "Believable-->Believable\n",
      "Respectful-->Respectful\n",
      "Manageable-->Manageable\n",
      "Doubtful-->Doubtful\n",
      "Interpretation-->Interpretation\n",
      "Estimation-->Estimation\n",
      "Preparedness-->Preparedness\n",
      "Cooked-->Cooked\n",
      "Successful-->Successful\n",
      "Consideration-->Consideration\n",
      "Production-->Production\n",
      "Approval-->Approval\n",
      "Realistic-->Realistic\n",
      "Statement-->Statement\n",
      "Writing-->Writing\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl=WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(word+\"-->\"+wl.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e91496a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"eating\",pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8d6ce",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de384ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aefce565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cc13981",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=WordNetLemmatizer()\n",
    "twt= TreebankWordTokenizer()\n",
    "temp=twt.tokenize(paragraph)\n",
    "temp2=[]\n",
    "for i in range(len(temp)):\n",
    "    if(temp[i] not in stop_words):\n",
    "        temp2.append(wnl.lemmatize(temp[i],pos='v'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcbf3ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'three',\n",
       " 'visions',\n",
       " 'India.',\n",
       " 'In',\n",
       " '3000',\n",
       " 'years',\n",
       " 'history',\n",
       " ',',\n",
       " 'people',\n",
       " 'world',\n",
       " 'come',\n",
       " 'invade',\n",
       " 'us',\n",
       " ',',\n",
       " 'capture',\n",
       " 'land',\n",
       " ',',\n",
       " 'conquer',\n",
       " 'minds.',\n",
       " 'From',\n",
       " 'Alexander',\n",
       " 'onwards',\n",
       " ',',\n",
       " 'Greeks',\n",
       " ',',\n",
       " 'Turks',\n",
       " ',',\n",
       " 'Moguls',\n",
       " ',',\n",
       " 'Portuguese',\n",
       " ',',\n",
       " 'British',\n",
       " ',',\n",
       " 'French',\n",
       " ',',\n",
       " 'Dutch',\n",
       " ',',\n",
       " 'come',\n",
       " 'loot',\n",
       " 'us',\n",
       " ',',\n",
       " 'take',\n",
       " 'ours.',\n",
       " 'Yet',\n",
       " 'do',\n",
       " 'nation.',\n",
       " 'We',\n",
       " 'conquer',\n",
       " 'anyone.',\n",
       " 'We',\n",
       " 'grab',\n",
       " 'land',\n",
       " ',',\n",
       " 'culture',\n",
       " ',',\n",
       " 'history',\n",
       " 'try',\n",
       " 'enforce',\n",
       " 'way',\n",
       " 'life',\n",
       " 'them.',\n",
       " 'Why',\n",
       " '?',\n",
       " 'Because',\n",
       " 'respect',\n",
       " 'freedom',\n",
       " 'others.That',\n",
       " 'first',\n",
       " 'vision',\n",
       " 'freedom.',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'India',\n",
       " 'get',\n",
       " 'first',\n",
       " 'vision',\n",
       " '1857',\n",
       " ',',\n",
       " 'start',\n",
       " 'War',\n",
       " 'Independence.',\n",
       " 'It',\n",
       " 'freedom',\n",
       " 'must',\n",
       " 'protect',\n",
       " 'nurture',\n",
       " 'build',\n",
       " 'on.',\n",
       " 'If',\n",
       " 'free',\n",
       " ',',\n",
       " 'one',\n",
       " 'respect',\n",
       " 'us.',\n",
       " 'My',\n",
       " 'second',\n",
       " 'vision',\n",
       " 'India’s',\n",
       " 'development.',\n",
       " 'For',\n",
       " 'fifty',\n",
       " 'years',\n",
       " 'develop',\n",
       " 'nation.',\n",
       " 'It',\n",
       " 'time',\n",
       " 'see',\n",
       " 'develop',\n",
       " 'nation.',\n",
       " 'We',\n",
       " 'among',\n",
       " 'top',\n",
       " '5',\n",
       " 'nations',\n",
       " 'world',\n",
       " 'term',\n",
       " 'GDP.',\n",
       " 'We',\n",
       " '10',\n",
       " 'percent',\n",
       " 'growth',\n",
       " 'rate',\n",
       " 'areas.',\n",
       " 'Our',\n",
       " 'poverty',\n",
       " 'level',\n",
       " 'falling.',\n",
       " 'Our',\n",
       " 'achievements',\n",
       " 'globally',\n",
       " 'recognise',\n",
       " 'today.',\n",
       " 'Yet',\n",
       " 'lack',\n",
       " 'self-confidence',\n",
       " 'see',\n",
       " 'develop',\n",
       " 'nation',\n",
       " ',',\n",
       " 'self-reliant',\n",
       " 'self-assured.',\n",
       " 'Isn’t',\n",
       " 'incorrect',\n",
       " '?',\n",
       " 'I',\n",
       " 'third',\n",
       " 'vision.',\n",
       " 'India',\n",
       " 'must',\n",
       " 'stand',\n",
       " 'world.',\n",
       " 'Because',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'unless',\n",
       " 'India',\n",
       " 'stand',\n",
       " 'world',\n",
       " ',',\n",
       " 'one',\n",
       " 'respect',\n",
       " 'us.',\n",
       " 'Only',\n",
       " 'strength',\n",
       " 'respect',\n",
       " 'strength.',\n",
       " 'We',\n",
       " 'must',\n",
       " 'strong',\n",
       " 'military',\n",
       " 'power',\n",
       " 'also',\n",
       " 'economic',\n",
       " 'power.',\n",
       " 'Both',\n",
       " 'must',\n",
       " 'go',\n",
       " 'hand-in-hand.',\n",
       " 'My',\n",
       " 'good',\n",
       " 'fortune',\n",
       " 'work',\n",
       " 'three',\n",
       " 'great',\n",
       " 'minds.',\n",
       " 'Dr.',\n",
       " 'Vikram',\n",
       " 'Sarabhai',\n",
       " 'Dept.',\n",
       " 'space',\n",
       " ',',\n",
       " 'Professor',\n",
       " 'Satish',\n",
       " 'Dhawan',\n",
       " ',',\n",
       " 'succeed',\n",
       " 'Dr.',\n",
       " 'Brahm',\n",
       " 'Prakash',\n",
       " ',',\n",
       " 'father',\n",
       " 'nuclear',\n",
       " 'material.',\n",
       " 'I',\n",
       " 'lucky',\n",
       " 'work',\n",
       " 'three',\n",
       " 'closely',\n",
       " 'consider',\n",
       " 'great',\n",
       " 'opportunity',\n",
       " 'life.',\n",
       " 'I',\n",
       " 'see',\n",
       " 'four',\n",
       " 'milestones',\n",
       " 'career']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6adabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "twdt=TreebankWordDetokenizer()\n",
    "temp3=twdt.detokenize(temp2)\n",
    "temp3=temp3.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac940c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2fd82c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three visions india.',\n",
       " 'in 3000 years history, people world come invade us, capture land, conquer minds.',\n",
       " 'from alexander onwards, greeks, turks, moguls, portuguese, british, french, dutch, come loot us, take ours.',\n",
       " 'yet do nation.',\n",
       " 'we conquer anyone.',\n",
       " 'we grab land, culture, history try enforce way life them.',\n",
       " 'why?',\n",
       " 'because respect freedom others.that first vision freedom.',\n",
       " 'i believe india get first vision 1857, start war independence.',\n",
       " 'it freedom must protect nurture build on.',\n",
       " 'if free, one respect us.',\n",
       " 'my second vision india’s development.',\n",
       " 'for fifty years develop nation.',\n",
       " 'it time see develop nation.',\n",
       " 'we among top 5 nations world term gdp.',\n",
       " 'we 10 percent growth rate areas.',\n",
       " 'our poverty level falling.',\n",
       " 'our achievements globally recognise today.',\n",
       " 'yet lack self-confidence see develop nation, self-reliant self-assured.',\n",
       " 'isn’t incorrect?',\n",
       " 'i third vision.',\n",
       " 'india must stand world.',\n",
       " 'because i believe unless india stand world, one respect us.',\n",
       " 'only strength respect strength.',\n",
       " 'we must strong military power also economic power.',\n",
       " 'both must go hand-in-hand.',\n",
       " 'my good fortune work three great minds.',\n",
       " 'dr. vikram sarabhai dept.',\n",
       " 'space, professor satish dhawan, succeed dr. brahm prakash, father nuclear material.',\n",
       " 'i lucky work three closely consider great opportunity life.',\n",
       " 'i see four milestones career']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912218c5",
   "metadata": {},
   "source": [
    "# POS Tags and Name Entity Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "640d1337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S i/JJ three/CD visions/NNS india/VBP ./.)\n",
      "(S\n",
      "  in/IN\n",
      "  3000/CD\n",
      "  years/NNS\n",
      "  history/NN\n",
      "  ,/,\n",
      "  people/NNS\n",
      "  world/NN\n",
      "  come/VBP\n",
      "  invade/VBP\n",
      "  us/PRP\n",
      "  ,/,\n",
      "  capture/NN\n",
      "  land/NN\n",
      "  ,/,\n",
      "  conquer/VB\n",
      "  minds/NNS\n",
      "  ./.)\n",
      "(S\n",
      "  from/IN\n",
      "  alexander/NN\n",
      "  onwards/NNS\n",
      "  ,/,\n",
      "  greeks/NNS\n",
      "  ,/,\n",
      "  turks/NNS\n",
      "  ,/,\n",
      "  moguls/NNS\n",
      "  ,/,\n",
      "  portuguese/JJ\n",
      "  ,/,\n",
      "  british/JJ\n",
      "  ,/,\n",
      "  french/JJ\n",
      "  ,/,\n",
      "  dutch/VB\n",
      "  ,/,\n",
      "  come/VB\n",
      "  loot/NN\n",
      "  us/PRP\n",
      "  ,/,\n",
      "  take/VB\n",
      "  ours/NNS\n",
      "  ./.)\n",
      "(S yet/RB do/VBP nation/NN ./.)\n",
      "(S we/PRP conquer/VBP anyone/NN ./.)\n",
      "(S\n",
      "  we/PRP\n",
      "  grab/VBP\n",
      "  land/NN\n",
      "  ,/,\n",
      "  culture/NN\n",
      "  ,/,\n",
      "  history/NN\n",
      "  try/NN\n",
      "  enforce/NN\n",
      "  way/NN\n",
      "  life/NN\n",
      "  them/PRP\n",
      "  ./.)\n",
      "(S why/WRB ?/.)\n",
      "(S\n",
      "  because/IN\n",
      "  respect/NN\n",
      "  freedom/NN\n",
      "  others.that/IN\n",
      "  first/JJ\n",
      "  vision/NN\n",
      "  freedom/NN\n",
      "  ./.)\n",
      "(S\n",
      "  i/NN\n",
      "  believe/VBP\n",
      "  india/NN\n",
      "  get/NN\n",
      "  first/JJ\n",
      "  vision/NN\n",
      "  1857/CD\n",
      "  ,/,\n",
      "  start/VBP\n",
      "  war/NN\n",
      "  independence/NN\n",
      "  ./.)\n",
      "(S\n",
      "  it/PRP\n",
      "  freedom/NN\n",
      "  must/MD\n",
      "  protect/VB\n",
      "  nurture/NN\n",
      "  build/NN\n",
      "  on/IN\n",
      "  ./.)\n",
      "(S if/IN free/JJ ,/, one/CD respect/NN us/PRP ./.)\n",
      "(S\n",
      "  my/PRP$\n",
      "  second/JJ\n",
      "  vision/NN\n",
      "  india/NN\n",
      "  ’/NNP\n",
      "  s/NN\n",
      "  development/NN\n",
      "  ./.)\n",
      "(S for/IN fifty/JJ years/NNS develop/VB nation/NN ./.)\n",
      "(S it/PRP time/NN see/VB develop/JJ nation/NN ./.)\n",
      "(S\n",
      "  we/PRP\n",
      "  among/IN\n",
      "  top/JJ\n",
      "  5/CD\n",
      "  nations/NNS\n",
      "  world/NN\n",
      "  term/NN\n",
      "  gdp/NN\n",
      "  ./.)\n",
      "(S we/PRP 10/CD percent/JJ growth/NN rate/NN areas/NNS ./.)\n",
      "(S our/PRP$ poverty/NN level/NN falling/VBG ./.)\n",
      "(S our/PRP$ achievements/NNS globally/RB recognise/VBP today/NN ./.)\n",
      "(S\n",
      "  yet/RB\n",
      "  lack/JJ\n",
      "  self-confidence/NN\n",
      "  see/NN\n",
      "  develop/VB\n",
      "  nation/NN\n",
      "  ,/,\n",
      "  self-reliant/JJ\n",
      "  self-assured/JJ\n",
      "  ./.)\n",
      "(S isn/NN ’/NNP t/NN incorrect/NN ?/.)\n",
      "(S i/JJ third/JJ vision/NN ./.)\n",
      "(S india/NN must/MD stand/VB world/NN ./.)\n",
      "(S\n",
      "  because/IN\n",
      "  i/JJ\n",
      "  believe/VBP\n",
      "  unless/IN\n",
      "  india/JJ\n",
      "  stand/NN\n",
      "  world/NN\n",
      "  ,/,\n",
      "  one/CD\n",
      "  respect/NN\n",
      "  us/PRP\n",
      "  ./.)\n",
      "(S only/RB strength/NN respect/JJ strength/NN ./.)\n",
      "(S\n",
      "  we/PRP\n",
      "  must/MD\n",
      "  strong/JJ\n",
      "  military/JJ\n",
      "  power/NN\n",
      "  also/RB\n",
      "  economic/JJ\n",
      "  power/NN\n",
      "  ./.)\n",
      "(S both/DT must/MD go/VB hand-in-hand/NN ./.)\n",
      "(S\n",
      "  my/PRP$\n",
      "  good/JJ\n",
      "  fortune/NN\n",
      "  work/NN\n",
      "  three/CD\n",
      "  great/JJ\n",
      "  minds/NNS\n",
      "  ./.)\n",
      "(S dr./NN vikram/NN sarabhai/NN dept/NN ./.)\n",
      "(S\n",
      "  space/NN\n",
      "  ,/,\n",
      "  professor/NN\n",
      "  satish/JJ\n",
      "  dhawan/NN\n",
      "  ,/,\n",
      "  succeed/VB\n",
      "  dr./JJ\n",
      "  brahm/NN\n",
      "  prakash/NN\n",
      "  ,/,\n",
      "  father/RB\n",
      "  nuclear/JJ\n",
      "  material/NN\n",
      "  ./.)\n",
      "(S\n",
      "  i/NN\n",
      "  lucky/JJ\n",
      "  work/NN\n",
      "  three/CD\n",
      "  closely/RB\n",
      "  consider/VBP\n",
      "  great/JJ\n",
      "  opportunity/NN\n",
      "  life/NN\n",
      "  ./.)\n",
      "(S i/NNS see/VBP four/CD milestones/NNS career/NN)\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578c2634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dac7610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\shubh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0b28f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three visions india.',\n",
       " 'in 3000 years history, people world come invade us, capture land, conquer minds.',\n",
       " 'from alexander onwards, greeks, turks, moguls, portuguese, british, french, dutch, come loot us, take ours.',\n",
       " 'yet do nation.',\n",
       " 'we conquer anyone.',\n",
       " 'we grab land, culture, history try enforce way life them.',\n",
       " 'why?',\n",
       " 'because respect freedom others.that first vision freedom.',\n",
       " 'i believe india get first vision 1857, start war independence.',\n",
       " 'it freedom must protect nurture build on.',\n",
       " 'if free, one respect us.',\n",
       " 'my second vision india’s development.',\n",
       " 'for fifty years develop nation.',\n",
       " 'it time see develop nation.',\n",
       " 'we among top 5 nations world term gdp.',\n",
       " 'we 10 percent growth rate areas.',\n",
       " 'our poverty level falling.',\n",
       " 'our achievements globally recognise today.',\n",
       " 'yet lack self-confidence see develop nation, self-reliant self-assured.',\n",
       " 'isn’t incorrect?',\n",
       " 'i third vision.',\n",
       " 'india must stand world.',\n",
       " 'because i believe unless india stand world, one respect us.',\n",
       " 'only strength respect strength.',\n",
       " 'we must strong military power also economic power.',\n",
       " 'both must go hand-in-hand.',\n",
       " 'my good fortune work three great minds.',\n",
       " 'dr. vikram sarabhai dept.',\n",
       " 'space, professor satish dhawan, succeed dr. brahm prakash, father nuclear material.',\n",
       " 'i lucky work three closely consider great opportunity life.',\n",
       " 'i see four milestones career']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac49ca8",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd02d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fef3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cv.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "508ad525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 135)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29e0b8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'three': 116,\n",
       " 'visions': 126,\n",
       " 'india': 59,\n",
       " 'in': 56,\n",
       " '3000': 2,\n",
       " 'years': 133,\n",
       " 'history': 54,\n",
       " 'people': 88,\n",
       " 'world': 132,\n",
       " 'come': 19,\n",
       " 'invade': 60,\n",
       " 'us': 123,\n",
       " 'capture': 16,\n",
       " 'land': 64,\n",
       " 'conquer': 21,\n",
       " 'minds': 72,\n",
       " 'from': 43,\n",
       " 'alexander': 4,\n",
       " 'onwards': 83,\n",
       " 'greeks': 51,\n",
       " 'turks': 121,\n",
       " 'moguls': 73,\n",
       " 'portuguese': 90,\n",
       " 'british': 14,\n",
       " 'french': 42,\n",
       " 'dutch': 30,\n",
       " 'loot': 67,\n",
       " 'take': 111,\n",
       " 'ours': 87,\n",
       " 'yet': 134,\n",
       " 'do': 28,\n",
       " 'nation': 76,\n",
       " 'we': 129,\n",
       " 'anyone': 7,\n",
       " 'grab': 49,\n",
       " 'culture': 23,\n",
       " 'try': 120,\n",
       " 'enforce': 32,\n",
       " 'way': 128,\n",
       " 'life': 66,\n",
       " 'them': 114,\n",
       " 'why': 130,\n",
       " 'because': 10,\n",
       " 'respect': 99,\n",
       " 'freedom': 41,\n",
       " 'others': 85,\n",
       " 'that': 113,\n",
       " 'first': 36,\n",
       " 'vision': 125,\n",
       " 'believe': 11,\n",
       " 'get': 45,\n",
       " '1857': 1,\n",
       " 'start': 107,\n",
       " 'war': 127,\n",
       " 'independence': 58,\n",
       " 'it': 62,\n",
       " 'must': 74,\n",
       " 'protect': 95,\n",
       " 'nurture': 79,\n",
       " 'build': 15,\n",
       " 'on': 80,\n",
       " 'if': 55,\n",
       " 'free': 40,\n",
       " 'one': 81,\n",
       " 'my': 75,\n",
       " 'second': 102,\n",
       " 'development': 26,\n",
       " 'for': 37,\n",
       " 'fifty': 35,\n",
       " 'develop': 25,\n",
       " 'time': 117,\n",
       " 'see': 103,\n",
       " 'among': 6,\n",
       " 'top': 119,\n",
       " 'nations': 77,\n",
       " 'term': 112,\n",
       " 'gdp': 44,\n",
       " '10': 0,\n",
       " 'percent': 89,\n",
       " 'growth': 52,\n",
       " 'rate': 96,\n",
       " 'areas': 8,\n",
       " 'our': 86,\n",
       " 'poverty': 91,\n",
       " 'level': 65,\n",
       " 'falling': 33,\n",
       " 'achievements': 3,\n",
       " 'globally': 46,\n",
       " 'recognise': 97,\n",
       " 'today': 118,\n",
       " 'lack': 63,\n",
       " 'self': 104,\n",
       " 'confidence': 20,\n",
       " 'reliant': 98,\n",
       " 'assured': 9,\n",
       " 'isn': 61,\n",
       " 'incorrect': 57,\n",
       " 'third': 115,\n",
       " 'stand': 106,\n",
       " 'unless': 122,\n",
       " 'only': 82,\n",
       " 'strength': 108,\n",
       " 'strong': 109,\n",
       " 'military': 71,\n",
       " 'power': 92,\n",
       " 'also': 5,\n",
       " 'economic': 31,\n",
       " 'both': 12,\n",
       " 'go': 47,\n",
       " 'hand': 53,\n",
       " 'good': 48,\n",
       " 'fortune': 38,\n",
       " 'work': 131,\n",
       " 'great': 50,\n",
       " 'dr': 29,\n",
       " 'vikram': 124,\n",
       " 'sarabhai': 100,\n",
       " 'dept': 24,\n",
       " 'space': 105,\n",
       " 'professor': 94,\n",
       " 'satish': 101,\n",
       " 'dhawan': 27,\n",
       " 'succeed': 110,\n",
       " 'brahm': 13,\n",
       " 'prakash': 93,\n",
       " 'father': 34,\n",
       " 'nuclear': 78,\n",
       " 'material': 69,\n",
       " 'lucky': 68,\n",
       " 'closely': 18,\n",
       " 'consider': 22,\n",
       " 'opportunity': 84,\n",
       " 'four': 39,\n",
       " 'milestones': 70,\n",
       " 'career': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed41ebb",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9fd9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04ffdfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.30538596, ..., 0.23121346, 0.2725641 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d950250",
   "metadata": {},
   "source": [
    "# Avg Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9c01cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8339d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wordvec', 'is', 'a', 'twolayer', 'neural', 'network', 'that', 'processes', 'text', 'by', 'vectorizing', 'words', 'it', 'creates', 'a', 'distributed', 'representation', 'of', 'words', 'in', 'a', 'continuous', 'vector', 'space', 'this', 'vectorization', 'captures', 'semantic', 'meanings', 'and', 'relationships', 'between', 'words']]\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "text_data = \"\"\"Word2Vec is a two-layer neural network that processes text by vectorizing words.\n",
    "It creates a distributed representation of words in a continuous vector space.\n",
    "This vectorization captures semantic meanings and relationships between words.\"\"\"\n",
    "\n",
    "# Clean and preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = text.split('.')\n",
    "    # Further tokenize sentences into words\n",
    "    sentences = [sentence.split() for sentence in sentences if sentence]\n",
    "    return sentences\n",
    "\n",
    "sentences = preprocess_text(text_data)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c9e2722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 330)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model parameters\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train the model (this step might take some time depending on your data size)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3a370dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'wordvec':\n",
      "[ 8.3421413e-03 -5.6057045e-04 -9.4266217e-03  4.8036375e-03\n",
      " -6.0469098e-03  6.6633029e-03  5.4016816e-03 -5.0190161e-03\n",
      "  2.5571322e-03  5.3904331e-03 -3.5782254e-03 -1.5304347e-03\n",
      "  9.1754841e-03  9.0766456e-03 -9.3987584e-03  7.5720684e-03\n",
      "  9.9075874e-03 -2.8468892e-03  2.4449273e-03 -2.8304826e-03\n",
      "  8.6586839e-03 -2.7284515e-04  5.6438819e-03  9.2259059e-03\n",
      "  4.0976922e-03 -7.1175927e-03 -1.9390055e-03  9.7679847e-04\n",
      "  2.0351645e-03  2.9583501e-03  9.4593875e-03  4.3971897e-03\n",
      "  9.9129770e-03 -8.6776828e-03 -5.7564965e-03  2.0046129e-03\n",
      "  3.6763209e-03 -1.0010966e-03 -6.9193272e-03 -3.2118317e-03\n",
      " -8.5174944e-03  9.4002914e-03  3.7095414e-03 -7.8835171e-03\n",
      "  3.1910953e-03  4.1679582e-03 -5.6474870e-03 -5.9222495e-03\n",
      "  1.0558590e-03  8.9738313e-03 -9.6357223e-03 -2.0450461e-06\n",
      " -6.8792636e-03 -9.2861842e-04  3.0487794e-03 -5.0131883e-03\n",
      " -2.7576168e-03  6.6817028e-04 -6.3689714e-03  7.2922478e-03\n",
      "  4.3668635e-03 -8.5652936e-03 -2.1403998e-03  3.1618809e-03\n",
      " -8.3285691e-03 -7.0499885e-03 -8.4693953e-03 -5.4976088e-03\n",
      "  8.8439742e-03  7.0809661e-03  2.8924553e-03 -8.5450960e-03\n",
      "  5.7529365e-03  4.6264194e-03  1.1811075e-04 -8.8532753e-03\n",
      " -1.8736611e-03  2.0439722e-04 -7.7532609e-03  2.4729420e-03\n",
      "  4.6683097e-04 -7.0494269e-03 -8.2545765e-03  6.0455939e-03\n",
      " -8.3516622e-03 -5.5856644e-03  5.5893003e-03 -4.8477107e-04\n",
      " -3.0437571e-03 -5.1925988e-03 -1.1773534e-03  5.2684564e-03\n",
      " -5.9482967e-03 -4.9768719e-03 -4.9340161e-03 -4.7688484e-03\n",
      " -7.9828184e-03 -9.7682122e-03  7.5786072e-03  8.0102459e-03]\n",
      "Vocabulary: ['a', 'words', 'between', 'creates', 'is', 'twolayer', 'neural', 'network', 'that', 'processes', 'text', 'by', 'vectorizing', 'it', 'distributed', 'relationships', 'representation', 'of', 'in', 'continuous', 'vector', 'space', 'this', 'vectorization', 'captures', 'semantic', 'meanings', 'and', 'wordvec']\n"
     ]
    }
   ],
   "source": [
    "# Get the vector for a specific word\n",
    "word = 'wordvec'\n",
    "vector = model.wv[word]\n",
    "print(f\"Vector for '{word}':\\n{vector}\")\n",
    "\n",
    "# List all words in the model's vocabulary\n",
    "words = list(model.wv.index_to_key)\n",
    "print(\"Vocabulary:\", words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478b5db",
   "metadata": {},
   "source": [
    "Basically Covert Corpus to Word\n",
    "\n",
    "then Remove Stop Words\n",
    "\n",
    "then train on Word2Vec Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
